{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries for data visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import the Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Import the OneHotEncoder to convert all the categorical columns; Note that this can only be done with int type columns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# # Import the Imputer to fill all missing values\n",
    "# from sklearn.preprocessing import Imputer\n",
    "\n",
    "# Import the libraries for bulding the model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Import the libraries for testing and metric measurements\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:466: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "# Import the final working data\n",
    "final_df = pd.read_csv('final_data.csv', index_col = 0)\n",
    "\n",
    "# Import the final test data\n",
    "final_test = pd.read_csv('final_test_data.csv',index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variables of interest\n",
    "interest_df = final_df[['shop_id', 'item_id', 'month', 'year', 'item_cnt_day']]\n",
    "\n",
    "# Group the data together to get months and take the mean to reduce the effects of the outliers.\n",
    "group_df = interest_df.groupby(['year', 'month', 'shop_id', 'item_id'], as_index=False).sum()\n",
    "\n",
    "# Fill all the nan values in the item_cnt_day column with 0's\n",
    "# group_df['item_cnt_day'].fillna(0, inplace=True)\n",
    "\n",
    "#Clip the results between 0 to 20 because the remainder is likely an anamoly\n",
    "group_df['item_cnt_day'].clip(0, 20, inplace=True)\n",
    "\n",
    "# Split the data into X (variables) and y (target)\n",
    "X = group_df.drop('item_cnt_day', axis=1)\n",
    "y = group_df['item_cnt_day']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=21, stratify=group_df['month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restructure the test data for columns of interest\n",
    "final_test = final_test[['year', 'month', 'shop_id', 'item_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1609487 entries, 0 to 1609486\n",
      "Data columns (total 5 columns):\n",
      "year            1609487 non-null int64\n",
      "month           1609487 non-null int64\n",
      "shop_id         1609487 non-null float64\n",
      "item_id         1609487 non-null int64\n",
      "item_cnt_day    1609487 non-null float64\n",
      "dtypes: float64(2), int64(3)\n",
      "memory usage: 113.7 MB\n"
     ]
    }
   ],
   "source": [
    "group_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate root mean squared error using lambda\n",
    "RMSE = (lambda x, y: np.sqrt(mean_squared_error(x, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the linear regression classifier\n",
    "lrClassifier = LinearRegression()\n",
    "\n",
    "# Initiate the OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "\n",
    "# Define the pipeline\n",
    "lrpipeline = Pipeline(steps=[\n",
    "    ('encoder', encoder), \n",
    "     ('lr', lrClassifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline with the training data\n",
    "lrModel = lrpipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the predictions to variables\n",
    "predicted_train_values = lrModel.predict(X_train)\n",
    "predicted_test_values = lrModel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data    \n",
      "Root Mean Squared Error: 2.0683230830555863\n",
      "R^2: 0.35584898277305343\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the RMSE and R^2 value for the model with the training data\n",
    "trainRMSE = RMSE(y_train, predicted_train_values)\n",
    "\n",
    "print ('Training Data    \\nRoot Mean Squared Error:', trainRMSE)\n",
    "print ('R^2:', lrModel.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data    \n",
      "Root Mean Squared Error: 2.090890961125138\n",
      "R^2: 0.34339069238455144\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the RMSE and R^2 value for the model with the test data\n",
    "testRMSE = RMSE(y_test, predicted_test_values)\n",
    "\n",
    "print ('Test Data    \\nRoot Mean Squared Error:', testRMSE)\n",
    "print ('R^2:', lrModel.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with the pipeline model\n",
    "pipelineResult = lrModel.predict(final_test)\n",
    "lrpipeResult = pd.DataFrame(pipelineResult, columns=['item_cnt_month'])\n",
    "lrpipeResult.to_csv('pipelineResult.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the SGDRegressor (best for working with large datasets like this) and XGBRegresor from xgboost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try the recommended algorithm for working with large datasets in regression according to:\n",
    "# http://scikit-learn.org/stable/tutorial/machine_learning_map/\n",
    "\n",
    "# Import SGDRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# Initiate the OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Instantiate the regressor with default parameters\n",
    "sgd_reg = SGDRegressor()\n",
    "\n",
    "# Apply the classifier into the pipeline with the OneHotEncoder step\n",
    "sgdPipeline = Pipeline(steps=[\n",
    "        ('encoder', encoder),\n",
    "        ('reg', sgd_reg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Fit the pipeline with the training data.\n",
    "sgdModel = sgdPipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the predictions to variables\n",
    "predicted_train_values = sgdModel.predict(X_train)\n",
    "predicted_test_values = sgdModel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data    \n",
      "Root Mean Squared Error: 2.3209058648989775\n",
      "R^2: 0.18891570682426706\n",
      "Test Data    \n",
      "Root Mean Squared Error: 2.322461802992094\n",
      "R^2: 0.18989479178500412\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the RMSE and R^2 value for the model with the training data\n",
    "trainRMSE = RMSE(y_train, predicted_train_values)\n",
    "\n",
    "print ('Training Data    \\nRoot Mean Squared Error:', trainRMSE)\n",
    "print ('R^2:', sgdModel.score(X_train, y_train))\n",
    "\n",
    "# Let's look at the RMSE and R^2 value for the model with the test data\n",
    "testRMSE = RMSE(y_test, predicted_test_values)\n",
    "\n",
    "print ('Test Data    \\nRoot Mean Squared Error:', testRMSE)\n",
    "print ('R^2:', sgdModel.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the model only performs slightly better than the linear model. However, it is much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with the pipeline model\n",
    "pipelineResult = sgdModel.predict(final_test)\n",
    "sgdpipeResult = pd.DataFrame(pipelineResult, columns=['item_cnt_month'])\n",
    "sgdpipeResult.to_csv('sgdpipelineResult.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both the linear regression and SGDRegressor algorithms, the score 1.45. I don't expecting tuning the paramets would lower the score an additional 0.45. As a result, I will try XGBoost here.\n",
    "\n",
    "An additional thing to note is that the score is higher, the lower the predictions are. It appears that the predicted values are overestimated the real values. This is why the clip(0,20) yields a higher score than without. Additionally, fitting without grouping (item_cnt_day), the model performs better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import xgboost as xgb\n",
    "import xgboost as xgb\n",
    "\n",
    "# Instantiate the XGBRegressor\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:linear')\n",
    "\n",
    "# Instantiate the OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Apply the regressor into the pipeline with the OneHotEncoder step\n",
    "xgbPipeline = Pipeline(steps=[\n",
    "        ('encoder', encoder),\n",
    "        ('reg', xg_reg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline with the training data.\n",
    "xgbModel = xgbPipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the predictions to variables\n",
    "predicted_train_values = xgbModel.predict(X_train)\n",
    "predicted_test_values = xgbModel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data    \n",
      "Root Mean Squared Error: 2.4056053724645907\n",
      "R^2: 0.12863580733924096\n",
      "Test Data    \n",
      "Root Mean Squared Error: 2.407892950538366\n",
      "R^2: 0.12919961003012925\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the RMSE and R^2 value for the model with the training data\n",
    "trainRMSE = RMSE(y_train, predicted_train_values)\n",
    "\n",
    "print ('Training Data    \\nRoot Mean Squared Error:', trainRMSE)\n",
    "print ('R^2:', xgbModel.score(X_train, y_train))\n",
    "\n",
    "# Let's look at the RMSE and R^2 value for the model with the test data\n",
    "testRMSE = RMSE(y_test, predicted_test_values)\n",
    "\n",
    "print ('Test Data    \\nRoot Mean Squared Error:', testRMSE)\n",
    "print ('R^2:', xgbModel.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineResult = xgbModel.predict(final_test)\n",
    "xgbpipeResult = pd.DataFrame(pipelineResult, columns=['item_cnt_month'])\n",
    "xgbpipeResult.to_csv('xgbpipelineResult.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost doesn't perform any better than SGDRegressor so I'll try some hyperparameter tuning here, particularly max_depth and learning_rate/eta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_dmatrix = xgb.DMatrix(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   max_depth      rmse\n",
      "0          2  2.564907\n",
      "1          5  2.478322\n",
      "2         10  2.299199\n"
     ]
    }
   ],
   "source": [
    "# Create the initial parameter dictionary\n",
    "params = {'objective':'reg:linear'}\n",
    "\n",
    "# Define the paramters for max_depths\n",
    "max_depths = [2, 5, 10]\n",
    "\n",
    "# Empty list to store the rmse of each fitting\n",
    "rmse_scores = []\n",
    "\n",
    "# Iterate over the max_depths list\n",
    "for val in max_depths:\n",
    "    \n",
    "    # Add the max_depth value to the parameters\n",
    "    params['max_depth'] = val\n",
    "    \n",
    "    # Pass the param to the cv\n",
    "    cv_results = xgb.cv(dtrain=sales_dmatrix, params=params, nfold=3,\n",
    "                       num_boost_round=5, metrics='rmse', as_pandas=True)\n",
    "    \n",
    "    # Append the best scores to the empty list\n",
    "    rmse_scores.append(cv_results['test-rmse-mean'].tail(1).values[0])\n",
    "    \n",
    "# Print the scores\n",
    "print (pd.DataFrame(list(zip(max_depths, rmse_scores)), columns=['max_depth', 'rmse']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   num_rounds      rmse\n",
      "0        0.01  2.890134\n",
      "1        0.10  2.424642\n",
      "2        0.20  2.258828\n",
      "3        0.30  2.174129\n"
     ]
    }
   ],
   "source": [
    "# Create the initial parameter dictionary\n",
    "params = {'objective':'reg:linear', 'max_depth': 10}\n",
    "\n",
    "# Define the paramters for max_depths\n",
    "eta_vals = [0.01, 0.1, 0.2, 0.3]\n",
    "\n",
    "# Empty list to store the rmse of each fitting\n",
    "rmse_scores = []\n",
    "\n",
    "# Iterate over the num_rounds list\n",
    "for val in eta_vals:\n",
    "    \n",
    "    # Add the eta value to the parameters\n",
    "    params['eta'] = val\n",
    "    \n",
    "    # Pass the param to the cv\n",
    "    cv_results = xgb.cv(dtrain=sales_dmatrix, params=params, nfold=3,\n",
    "                       num_boost_round=10, metrics='rmse', as_pandas=True)\n",
    "    \n",
    "    # Append the best scores to the empty list\n",
    "    rmse_scores.append(cv_results['test-rmse-mean'].tail(1).values[0])\n",
    "    \n",
    "# Print the scores\n",
    "print (pd.DataFrame(list(zip(eta_vals, rmse_scores)), columns=['num_rounds', 'rmse']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the XGBRegressor with the new parameters\n",
    "xgb_reg = xgb.XGBRegressor(objective='reg:linear',\n",
    "                           max_depth=10, learning_rate=0.3)\n",
    "\n",
    "# Instantiate the OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Apply the regressor into the pipeline with the OneHotEncoder step\n",
    "xgbPipeline = Pipeline(steps=[\n",
    "        ('encoder', encoder),\n",
    "        ('reg', xgb_reg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline with the training data.\n",
    "xgbModel = xgbPipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data    \n",
      "Root Mean Squared Error: 1.9961284916948256\n",
      "R^2: 0.4000322223128612\n",
      "Test Data    \n",
      "Root Mean Squared Error: 2.0470695581207794\n",
      "R^2: 0.3706250342674622\n"
     ]
    }
   ],
   "source": [
    "# Assign the predictions to variables\n",
    "predicted_train_values = xgbModel.predict(X_train)\n",
    "predicted_test_values = xgbModel.predict(X_test)\n",
    "\n",
    "# Let's look at the RMSE and R^2 value for the model with the training data\n",
    "trainRMSE = RMSE(y_train, predicted_train_values)\n",
    "\n",
    "print ('Training Data    \\nRoot Mean Squared Error:', trainRMSE)\n",
    "print ('R^2:', xgbModel.score(X_train, y_train))\n",
    "\n",
    "# Let's look at the RMSE and R^2 value for the model with the test data\n",
    "testRMSE = RMSE(y_test, predicted_test_values)\n",
    "\n",
    "print ('Test Data    \\nRoot Mean Squared Error:', testRMSE)\n",
    "print ('R^2:', xgbModel.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning the parameters reduced the RMSE by approximate 0.4\n",
    "\n",
    "#Load the data in\n",
    "pipelineResult = xgbModel.predict(final_test)\n",
    "xgbpipeResult = pd.DataFrame(pipelineResult, columns=['item_cnt_month'])\n",
    "xgbpipeResult.to_csv('xgbtuned_pipelineResult.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, I would like to try more complex algorithms for my model. First I will try SupportVectorRegression from sklearn.svm. However, since a lot of these more complex algorithms are computationally intensive, I will try to reduce the amount of values to work with, or I will only use a small section of the total data for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following algorithms are too complex and computationally intensive to be used on the large dataset. As a result, I will be using a subset of the data for training. First we'll redefine the X, and y data then split it using 20% training data and 80% test data. This gives us approximately 320,000 entries for the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 20-80 split didn't work for the DecisionTreeClassifier, so I expect it not to work with the rest of the more complex algorithms. The 10-90 split does work, but the split leaves single entries out of the training data. I won't be exploring these algorithms any further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the DecisionTreeClassifier from sklearn.tree\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# # Initiate the OneHotEncoder\n",
    "# encoder = OneHotEncoder()\n",
    "\n",
    "# # Instantiate the classifier with default parameters\n",
    "# clf_dt = DecisionTreeClassifier()\n",
    "\n",
    "# # Apply the classifier into the pipeline with the OneHotEncoder step\n",
    "# dtPipeline = Pipeline(steps=[\n",
    "#         ('encoder', encoder),\n",
    "#         ('clf', clf_dt)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit the pipeline with the training data\n",
    "# dtModel = dtPipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the SupportVectorRegression function from SVM for polynomial regression\n",
    "# from sklearn import svm\n",
    "\n",
    "# # Initiate the SVR classifier with default parameters\n",
    "# svr = svm.SVR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the RandomForestRegressor from sklearn.ensemble\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# # Instantiate the classifier with default parameters\n",
    "# rf = RandomForestRegressor()\n",
    "\n",
    "# # Apply the regressor into a pipeline with the OneHotEncoder step\n",
    "# rfPipeline = Pipeline(steps=[\n",
    "#         ('encoder', encoder),\n",
    "#         ('rf', rf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit the pipeline with the training data\n",
    "# rfmodel = rfPipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
